{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shanaam/colab_tests/blob/main/projects/AriaEverydayActivities/examples/aea_quickstart_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MA9n_W-XcSAn"
      },
      "source": [
        "# AEA Tutorial\n",
        "In this tutorial we will walk through the steps to access [Aria Everyday Activities (AEA) dataset](https://www.projectaria.com/datasets/aea/) The AEA dataset contains:\n",
        "* Raw data from 1-2 [Project Aria glasses](https://projectaria.com)\n",
        "    * Data is synchronized where recordings are made with two Aria glasses in the same location\n",
        "* Speech to Text annotation\n",
        "* Machine Perception Services [(MPS)](https://facebookresearch.github.io/projectaria_tools/docs/ARK/mps) derived data:\n",
        "    * General Eye Gaze\n",
        "    * Trajectory\n",
        "        * Open loop trajectories\n",
        "        * Closed loop trajectories\n",
        "        * Calibration data\n",
        "    * 3D Point Clouds\n",
        "  \n",
        "This tutorial covers how to:\n",
        "* Access raw sensor data (VRS files)\n",
        "* Visualize Eye Gaze data\n",
        "* Visualize Speech data\n",
        "* Load concurrent sequences from multiple Project Aria glasses in a shared space location\n",
        "* Use Timecode to get synchronized timestamps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFvZ7M7WcSAp"
      },
      "outputs": [],
      "source": [
        "# Specifics for Google Colab\n",
        "google_colab_env = 'google.colab' in str(get_ipython())\n",
        "if google_colab_env:\n",
        "    print(\"Running from Google Colab, installing projectaria_tools and getting sample data\")\n",
        "    !pip install projectaria-tools>=1.3.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U4CiB3stcSAq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "from projectaria_tools.projects.aea import (\n",
        "    AriaEverydayActivitiesDataPathsProvider,\n",
        "    AriaEverydayActivitiesDataProvider)\n",
        "import numpy as np\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from projectaria_tools.core import calibration, mps\n",
        "from projectaria_tools.core.sensor_data import TimeDomain, TimeQueryOptions\n",
        "from projectaria_tools.core.mps import get_eyegaze_point_at_depth\n",
        "\n",
        "from projectaria_tools.core.sophus import SE3\n",
        "from projectaria_tools.core.stream_id import StreamId"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFcZGFAicSAu"
      },
      "source": [
        "## Create AEA data provider\n",
        "The AEA data provider is similar to the [Project Aria Tools data provider](https://facebookresearch.github.io/projectaria_tools/docs/data_utilities/core_code_snippets/data_provider), with the additional ability to access speech data and data from multiple concurrent recordings.\n",
        "\n",
        "* Create AEA data provider and access all the following providers\n",
        "    * mps provider\n",
        "    * speech provider\n",
        "    * vrs provider\n",
        "* Create AEA data_path_provider and access the specific file directory\n",
        "    * Get sequence meta data\n",
        "    * Get paths of vrs, mps, and speech\n",
        "    * Get path of concurrent recording"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSi8dG_4cSAu"
      },
      "outputs": [],
      "source": [
        "if google_colab_env:\n",
        "    aea_sample_path = \"./aea_sample_data\"\n",
        "else:\n",
        "    aea_sample_path = \"/tmp/aea_sample_data\"\n",
        "\n",
        "data_sequence_url_rec1 = \"https://www.projectaria.com/async/sample/download/?bucket=aea&filename=loc1_script2_seq1_rec1_10s_sample.zip\"\n",
        "data_sequence_url_rec2 = \"https://www.projectaria.com/async/sample/download/?bucket=aea&filename=loc1_script2_seq1_rec2_10s_sample.zip\"\n",
        "\n",
        "sequence_name_rec1 = \"loc1_script2_seq1_rec1_10s_sample\"\n",
        "sequence_name_rec2 = \"loc1_script2_seq1_rec2_10s_sample\"\n",
        "\n",
        "sequence_path_rec1 = f\"{aea_sample_path}/{sequence_name_rec1}\"\n",
        "sequence_path_rec2 = f\"{aea_sample_path}/{sequence_name_rec2}\"\n",
        "\n",
        "command_list = [\n",
        "    f\"mkdir -p {aea_sample_path}\",\n",
        "    f\"mkdir -p {sequence_path_rec1}\",\n",
        "    f\"mkdir -p {sequence_path_rec2}\",\n",
        "    # Download sample data\n",
        "    f'curl -o {aea_sample_path}/aea_sample_data_rec1.zip -C - -O -L \"{data_sequence_url_rec1}\"',\n",
        "    f'curl -o {aea_sample_path}/aea_sample_data_rec2.zip -C - -O -L \"{data_sequence_url_rec2}\"',\n",
        "    # Unzip the sample data\n",
        "    f\"unzip -o {aea_sample_path}/aea_sample_data_rec1.zip -d {sequence_path_rec1}\",\n",
        "    f\"unzip -o {aea_sample_path}/aea_sample_data_rec2.zip -d {sequence_path_rec2}\",\n",
        "\n",
        "]\n",
        "\n",
        "# Execute the commands for downloading dataset\n",
        "if google_colab_env:\n",
        "    for command in command_list:\n",
        "        !$command\n",
        "else:\n",
        "    for command in command_list:\n",
        "        subprocess.run(command, shell=True, check=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SKYj2Bw-cSAv"
      },
      "outputs": [],
      "source": [
        "# Create AEA path provider\n",
        "aea_paths_provider_1 = AriaEverydayActivitiesDataPathsProvider(sequence_path_rec1)\n",
        "\n",
        "# create AEA data provider\n",
        "aea_data_provider_1 = AriaEverydayActivitiesDataProvider(sequence_path_rec1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgxziEFKcSAv"
      },
      "source": [
        "## Show AEA data paths\n",
        "AEA data path from a sequence can be obtained using AEA path provider"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOZd_aAAcSAv"
      },
      "outputs": [],
      "source": [
        "print(aea_paths_provider_1.get_data_paths())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajKSaDFbcSAv"
      },
      "source": [
        "## Access concurrent recordings\n",
        "Query metadata.json to find and load the directory of any concurrent recording into the AEA data_path_provider.\n",
        "If your recording doesn’t have a concurrent recording you’ll get the error: “RuntimeError: Timedomain TimeCode not supported by stream RGB Camera Class #1”.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F8taBuqgcSAv"
      },
      "outputs": [],
      "source": [
        "# Get concurrent recording filename and load into AEA data provider\n",
        "concurrent_recording_filename = aea_paths_provider_1.get_concurrent_recordings()\n",
        "concurrent_sequence_path = os.path.join(os.path.dirname(sequence_path_rec1), concurrent_recording_filename[0])\n",
        "\n",
        "# Load concurrent sequence directly\n",
        "aea_data_provider_2 = AriaEverydayActivitiesDataProvider(concurrent_sequence_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JSneFbkfcSAw"
      },
      "source": [
        "## Access synchronized timestamp from both provider\n",
        "Timestamp can come from four different places:\n",
        "* TimeDomain.DEVICE_TIME\n",
        "* TimeDomain.TIME_CODE\n",
        "* TimeDomain.RECORD_TIME\n",
        "* TimeDomain.HOST_TIME\n",
        "\n",
        "**TimeDomain.TIME_CODE** is commonly used to register timestamp of multiple synchronized recordings.\n",
        "\n",
        "For AEA dataset, TimeDomain.TIME_CODE is used first to find match and TimeDomain.DeviceTime is used to query data from MPS and speech."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "495cTEWWcSAw"
      },
      "outputs": [],
      "source": [
        "RGB_STREAM_ID = StreamId(\"214-1\")\n",
        "\n",
        "# timecode timestamps for recording 1\n",
        "timecode_vec_1 = aea_data_provider_1.vrs.get_timestamps_ns(RGB_STREAM_ID, TimeDomain.TIME_CODE)\n",
        "\n",
        "# timecode timestamps for recording 2\n",
        "timecode_vec_2 = aea_data_provider_2.vrs.get_timestamps_ns(RGB_STREAM_ID, TimeDomain.TIME_CODE)\n",
        "\n",
        "# Convert the common timecode timestamp to unique DeviceTime timestamp for each sequence\n",
        "for timecode_time_ns in timecode_vec_1:\n",
        "        device_time_ns_1 = aea_data_provider_1.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
        "        device_time_ns_2 = aea_data_provider_2.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
        "        print(f\"TimeCode {timecode_time_ns} converts to DeviceTime of sequence 1: {device_time_ns_1} and sequence 2: {device_time_ns_2}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tfzb7SD-cSAw"
      },
      "source": [
        "## Access speech data\n",
        "Obtain speech data provider directly from aea_data_provider and query the speech data by sentence or words with timestamp in nanoseconds.\n",
        "* Speech data can be only be queried with `TimeDomain.DEVICE_TIME`\n",
        "* Timestamp conversion from / to `TimeDomain.TIME_CODE` using `convert_from_timecode_to_device_time_ns`\n",
        "* Obtain sentence/words from speech data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSshoXhccSAw"
      },
      "outputs": [],
      "source": [
        "timecode_timestamp = timecode_vec_1[100]\n",
        "device_time_ns_1 = aea_data_provider_1.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)\n",
        "device_time_ns_2 = aea_data_provider_2.vrs.convert_from_timecode_to_device_time_ns(timecode_time_ns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BEIo-KfcSAx"
      },
      "outputs": [],
      "source": [
        "# obtain sentence at query_time_ns\n",
        "sentence = aea_data_provider_1.speech.get_sentence_data_by_timestamp_ns(device_time_ns_1, TimeQueryOptions.CLOSEST)\n",
        "print(f\"Sentence from sequence 1 at device timestamp {device_time_ns_1} is : '{sentence}'\")\n",
        "\n",
        "# obtain word at query_time_ns\n",
        "sentence = aea_data_provider_2.speech.get_sentence_data_by_timestamp_ns(device_time_ns_2, TimeQueryOptions.CLOSEST)\n",
        "print(f\"Sentence from sequence 2 at device timestamp {device_time_ns_2} is : '{sentence}'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYyydybPcSAy"
      },
      "source": [
        "## Visualize RGB image with Gaze"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jss5BsfccSAy"
      },
      "outputs": [],
      "source": [
        "def draw_eye_gaze(aea_data_provider : AriaEverydayActivitiesDataProvider, device_time_ns: int, depth_m: float):\n",
        "    rgb_stream_label = aea_data_provider.vrs.get_label_from_stream_id(RGB_STREAM_ID)\n",
        "    device_calibration = aea_data_provider.vrs.get_device_calibration()\n",
        "    rgb_camera_calibration = device_calibration.get_camera_calib(rgb_stream_label)\n",
        "\n",
        "    image = aea_data_provider.vrs.get_image_data_by_time_ns(\n",
        "            RGB_STREAM_ID, device_time_ns, TimeDomain.DEVICE_TIME, TimeQueryOptions.BEFORE)\n",
        "    eye_gaze = aea_data_provider.mps.get_general_eyegaze(device_time_ns, TimeQueryOptions.CLOSEST)\n",
        "\n",
        "    assert eye_gaze, \"Eye gaze not available\"\n",
        "    # Compute eye_gaze vector at depth_m reprojection in the image\n",
        "    gaze_vector_in_cpf = mps.get_eyegaze_point_at_depth(\n",
        "        eye_gaze.yaw, eye_gaze.pitch, depth_m\n",
        "    )\n",
        "    T_device_CPF = device_calibration.get_transform_device_cpf()\n",
        "    gaze_center_in_camera = (\n",
        "        rgb_camera_calibration.get_transform_device_camera().inverse()\n",
        "        @ T_device_CPF\n",
        "        @ gaze_vector_in_cpf\n",
        "    )\n",
        "    gaze_projection = rgb_camera_calibration.project(gaze_center_in_camera)\n",
        "\n",
        "    # Project the gaze center in rgb camera frame\n",
        "    if gaze_projection is not None:\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.imshow(image[0].to_numpy_array());\n",
        "        ax.axis('off')\n",
        "\n",
        "        # Plot the cross\n",
        "        u, v = gaze_projection.flatten()\n",
        "        plt.plot(u, v, '+', c=\"red\", mew=1, ms=20); plt.xticks([]); plt.yticks([]);\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(f\"Eye gaze center projected to {gaze_projection}, which is out of camera sensor plane.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TlBdd5mUcSAy"
      },
      "outputs": [],
      "source": [
        "# draw general eye gaze with synchronized device timestamp at depth = 1.0 meter\n",
        "eye_gaze_depth = 1.0 # meters\n",
        "draw_eye_gaze(aea_data_provider_1, device_time_ns_1, eye_gaze_depth)\n",
        "draw_eye_gaze(aea_data_provider_2, device_time_ns_2, eye_gaze_depth)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ncQtUfBcSAy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "custom": {
      "cells": [],
      "metadata": {
        "custom": {
          "cells": [],
          "metadata": {
            "custom": {
              "cells": [],
              "metadata": {
                "fileHeader": "",
                "fileUid": "07f6fe44-5558-4c96-b69d-656c9e2fc4c2",
                "isAdHoc": false,
                "kernelspec": {
                  "display_name": "Python 3 (ipykernel)",
                  "language": "python",
                  "name": "python3"
                },
                "language_info": {
                  "codemirror_mode": {
                    "name": "ipython",
                    "version": 3
                  },
                  "file_extension": ".py",
                  "mimetype": "text/x-python",
                  "name": "python",
                  "nbconvert_exporter": "python",
                  "pygments_lexer": "ipython3",
                  "version": "3.11.5"
                }
              },
              "nbformat": 4,
              "nbformat_minor": 5
            },
            "fileHeader": "",
            "fileUid": "262809d5-bb0d-4d4c-a2ed-b85174a862f0",
            "indentAmount": 2,
            "isAdHoc": false,
            "kernelspec": {
              "display_name": "Python 3 (ipykernel)",
              "language": "python",
              "name": "python3"
            },
            "language_info": {
              "codemirror_mode": {
                "name": "ipython",
                "version": 3
              },
              "file_extension": ".py",
              "mimetype": "text/x-python",
              "name": "python",
              "nbconvert_exporter": "python",
              "pygments_lexer": "ipython3",
              "version": "3.11.5"
            }
          },
          "nbformat": 4,
          "nbformat_minor": 4
        },
        "fileHeader": "",
        "fileUid": "de0f180a-2fa5-4edf-8c65-a031c3f55d76",
        "indentAmount": 2,
        "isAdHoc": false,
        "language_info": {
          "name": "plaintext"
        }
      },
      "nbformat": 4,
      "nbformat_minor": 2
    },
    "indentAmount": 2,
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}